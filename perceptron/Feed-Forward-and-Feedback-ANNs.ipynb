{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-Linear Decision Boundries\n",
    "Example: XOR Gate\n",
    "<div>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*CyGlr8VjwtQGeNsuTUq3HA.jpeg\" alt=\"XOR Decision Boundry\" width=\"75%%\" style=\"display: inline-flex;\"/>\n",
    "</div>\n",
    "\n",
    "We get around this by representing an XOR using AND, NAND, and OR gates  \n",
    "$ a \\bigoplus b = (a + b) \\cdot (\\overline{a \\cdot b})$\n",
    "\n",
    "\n",
    "# Feed-Forward and Feedbak ANNs\n",
    "#### Components\n",
    "* Model's architecture/topology\n",
    "  * Describes the types of neuron\n",
    "  * Structure of the connections between them\n",
    "* Activation functions\n",
    "* Learning algorithm:\n",
    "  * Finds the optimal values of the weights\n",
    "  \n",
    "#### Feed-Forward Neural Networks\n",
    "* Defined by their acyclic graphs\n",
    "* Information travels in one directions towards output\n",
    "* Learn a function to map input to output\n",
    "\n",
    "#### Feedback (recurrent) Neural Networks\n",
    "* Contains cycles\n",
    "* Networks's behavior changes over time due to feedback\n",
    "* Processing sequences of inputs, translating documents and transcription\n",
    "* Not implemented in Scikit-Learn\n",
    "\n",
    "# Multi-Layer Perceptrons\n",
    "<img src=\"https://www.oreilly.com/library/view/getting-started-with/9781786468574/graphics/B05474_04_05.jpg\" alt=\"drawing\" width=\"75%\" style=\"display: inline-flex;\"/>\n",
    "\n",
    "#### Training MLPs\n",
    "* Use gradient descent to minimize a real-valued function, C\n",
    "  * $C = f(v_1, v_2)$\n",
    "  * A small cange in the variables produces a small change in the output\n",
    "  * Change in value of $v_1 = \\Delta v_1$\n",
    "  * Change in value of $v_2 = \\Delta v_2$\n",
    "  * Change in value of $C = \\Delta C$\n",
    "  * $\\Delta C = \\frac{\\delta C}{\\delta v_1}\\Delta v_1 +\n",
    "  \\frac{\\delta C}{\\delta v_2}\\Delta v_2$\n",
    "    * Represent as a vector: $\\Delta v = (\\Delta v_1, \\Delta v_2)^T$\n",
    "    * Gradient Vector of C:\n",
    "    $\\Delta C = (\\frac{\\delta C}{\\delta v_1},\\frac{\\delta C}{\\delta v_2})^T$\n",
    "    * Rewritten: $\\Delta C = \\nabla C \\Delta v$\n",
    "    * On each iteration, $\\Delta C$ should be negative to decrease the value\n",
    "    of the cost function. To do this we set $\\Delta v$ to the following:\n",
    "      * $\\Delta v = -\\eta\\nabla C$, where $\\eta$ (eta) is a hyperparameter called\n",
    "      the learning rate\n",
    "      * $\\Delta v = -\\eta\\nabla C \\cdot\\nabla C$, so $(\\nabla C)^2 > 0$\n",
    "  * Then Multiply $(\\nabla C)^2$ by the learning rate\n",
    "  * Negate the product\n",
    "  * Calculate gradient of $C$ on each iteration and update the variables\n",
    "* Calculating Error with Squared Error Cost Function\n",
    "  * $E = \\frac{1}{2}\\sum_{i=1}^n (y_i - \\hat{y_i})^2$\n",
    "  \n",
    "_*Note: The practical application of these equations is quite lengthy.*_\n",
    "_*It is recommended to look through an example that applies the activation*_\n",
    "_*function and then applies the given response to adjust weights.*_\n",
    "_*It's simply too much to write out in markdown here*_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "True: 0, Predicted: 0\n",
      "True: 1, Predicted: 1\n",
      "True: 1, Predicted: 1\n",
      "True: 0, Predicted: 0\n"
     ]
    }
   ],
   "source": [
    "# Training a MLP to approximate XOR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Define XOR data\n",
    "y = [0, 1, 1, 0]\n",
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "\n",
    "# Uses logistic sigmoid activation function\n",
    "# 2 units in one hidden layer\n",
    "clf = MLPClassifier(solver='lbfgs', activation='logistic',\n",
    "                    hidden_layer_sizes=(2,), max_iter=100,\n",
    "                    random_state=20)\n",
    "clf.fit(X, y)\n",
    "\n",
    "predictions = clf.predict(X)\n",
    "print('Accuracy: %s' % clf.score(X, y))\n",
    "for i, p in enumerate(predictions[:10]):\n",
    "    print('True: %s, Predicted: %s' % (y[i], p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights connecting the input layer and the hidden layer: \n",
      "[[6.11803938 6.35656371]\n",
      " [5.79147868 6.14551925]]\n",
      "Hidden layer bias weights: \n",
      "[-9.38637917 -2.77751758]\n",
      "Weights connecting the hiden layer and the output layer: \n",
      "[[-14.9548178]\n",
      " [ 14.5308097]]\n",
      "Output layer bias weight: \n",
      "[-7.22845316]\n"
     ]
    }
   ],
   "source": [
    "print('Weights connecting the input layer and the hidden layer: \\n%s' % clf.coefs_[0])\n",
    "print('Hidden layer bias weights: \\n%s' % clf.intercepts_[0])\n",
    "print('Weights connecting the hiden layer and the output layer: \\n%s' % clf.coefs_[1])\n",
    "print('Output layer bias weight: \\n%s' % clf.intercepts_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95348837 0.96160267 0.90604027]\n"
     ]
    }
   ],
   "source": [
    "# Training a multi-layer perceptron to classify handwritten digits\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "\n",
    "# main function is used to fork additional processes during cross-validation\n",
    "if __name__ == '__main__':\n",
    "    # load mnist dataset\n",
    "    digits = load_digits()\n",
    "    # Scaling the features. Particularly useful for ANNs to converge more quickly\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "    # Create a pipeline which scales the data before fitting an MLPClassifier\n",
    "    # MLP conntains: an input layer, two hidden layers with 150 and 100 units respectively,\n",
    "    # and an output layer. Increased the value of regularisation parameter (alpha) and\n",
    "    # increase the max iterations to 300\n",
    "    pipeline = Pipeline([\n",
    "        ('ss', StandardScaler()),\n",
    "        ('mlp', MLPClassifier(hidden_layer_sizes=(150, 100), alpha=0.1, max_iter=300, random_state=20))\n",
    "    ])\n",
    "    print(cross_val_score(pipeline, X, y, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Adding more hidden units or hidden layers or using gridsearch to tune the hyperparameters will help increase accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
