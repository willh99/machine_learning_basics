{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [The Perceptron](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=3&cad=rja&uact=8&ved=2ahUKEwiflsSE067dAhULx1kKHU2oAgEQFjACegQIChAL&url=https%3A%2F%2Ftowardsdatascience.com%2Fwhat-the-hell-is-perceptron-626217814f53&usg=AOvVaw2dGzhYaivl13eBWGSyhbzi)\n",
    "\n",
    "The perceptron is a linear binary classifier that is analagous to a neuron in\n",
    "that it takes in one or more inputs, processes it, and produces an output.\n",
    "\n",
    "### Online Learning\n",
    "* Learning algorithm can update models paremeters using:\n",
    "  * Single training instance rather the entire batch\n",
    "* Useful for learning from training sets too large to represent in memory\n",
    "\n",
    "### Activation Functions \n",
    "The perceptron classifies instances by proessing a linear combination of feaures and model parameters\n",
    "using an activation function  \n",
    "$y = \\phi(\\sum^n_{i=1} w_i x_i + b)$, where $w_i$ are the models parameters, *b* is a constant bias term,\n",
    "and $\\phi$ is the activation function\n",
    "\n",
    "The linear activation of the parameters and inputs is sometimes call __Preactivation__\n",
    "\n",
    "##### Heaviside step function\n",
    "Rosenbatt's orginal perceptron used a __Heaviside step function__ also called a unit step function  \n",
    "g(x) = 1, if x>0  \n",
    "g(x) = 0, otherwise\n",
    "\n",
    "##### Logistic Sigmoid\n",
    "$g(x) = \\frac{1}{1 + e^{-x}}$, where *x* is the weighted sum of the inputs. Note: it is differential\n",
    "\n",
    "#### Perceptron Learning Algorithm\n",
    "$w_i(t+1) = w_i(t) + \\alpha(d_j + y_j(t))x_{i,j}$, for all feature $0\\le i \\le n$  \n",
    "\n",
    "$d_j$ - true class for instance j  \n",
    "$y_j(t)$ - prediected class for instance j  \n",
    "$x_{i,j}$ - value of the $i^{th}$ feature for instance j  \n",
    "$\\alpha$ - hyperparamter that controlls the learning rate   \n",
    "\n",
    "* Set weights to zero/small random values\n",
    "* Predict class for training instances\n",
    "  * If correct: Continue to next instance without updating weights\n",
    "  * If incorrect: Update the weights\n",
    "    * Compute: $d_j + y_j(t) \\times$ *the value feature* $\\times$ *the learning rate*\n",
    "* Each pass through the training instances is called an epoch\n",
    "* The learning algorithm has converged when it complete an epoch without misclassifying\n",
    "any of the instances\n",
    "\n",
    "#### Document Classification\n",
    "* Fit\n",
    "* Predict\n",
    "* Partial fit - allows classifier to be trained incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.92      0.86       396\n",
      "          1       0.87      0.76      0.81       397\n",
      "          2       0.86      0.85      0.86       399\n",
      "\n",
      "avg / total       0.85      0.84      0.84      1192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.perceptron.Perceptron'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Document classification with the perceptron\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Fetch and read the dataset (fetch_20newsgroups)\n",
    "categories = ['rec.sport.hockey', 'rec.sport.baseball', 'rec.autos']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, \n",
    "                                     remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
    "X_test = vectorizer.transform(newsgroups_test.data)\n",
    "clf = Perceptron(random_state=11)\n",
    "clf.fit(X_train, newsgroups_train.target )\n",
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(newsgroups_test.target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limitations of the Perceptron\n",
    "\n",
    "* The perceptron uses a hyperplane to seporate positive and negative classes\n",
    "  * Example: XOR\n",
    "* In other words: Not possible for some datasets that are not linearly seporable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
