{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear Classification and Regression with Decision Trees\n",
    "\n",
    "### Decision Trees\n",
    "* Tree-like graphs that model a decision\n",
    "  * Learned by recursively splitting the set of training instances into subsets based on instances features\n",
    "  * Nodes are connected by edges that specify possible outcomes of the tests\n",
    "  * Children nodes test their subsets until reaching a stoppoing criterion\n",
    "  * Classification tasks - leaf nodes represent classes\n",
    "  * Regression tasks - values of the response variable produce the estimate\n",
    "* Making a prediction requires following edges until a leaf node is reached"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Decision Tree with Iterative Dichotomiser 3 (ID3)\n",
    "\n",
    "**Example:** You are tasked with classifying animals as cats or dogs. \n",
    "However, you cannot observe the animals directly, but instead must rely\n",
    "on only whether the animal likes the following attributes:\n",
    "* Playing fetch\n",
    "* Frequently grumpy\n",
    "* Favorite of three types of food  \n",
    "\n",
    "To classify new animals, examine a feature at each node in the decision tree.\n",
    "\n",
    "Tests that reduce entropy (uncertainty about the data) the most are best done first. \n",
    "In the example this may be whether the animal likes playing fetch since almost\n",
    "all dogs like fetch while almost no cats enjoy it.\n",
    "\n",
    "Entropy is given by: $H(X) = -\\sum_{i=1}^{n}P(x_i) log_2 P(x_i)$ **,**  \n",
    "where n is the number of outcomes and $P(x_i)$ is the probability of outcome *i*\n",
    "\n",
    "##### Information Gain -\n",
    "the difference between the entropy of the parent node and $H(T)$ and the weighted\n",
    "sum of the childrens' entropies. Higher information gain means a more desirable\n",
    "test. ID3 breaks ties by choosing one set arbitrarily.\n",
    "\n",
    "##### Gini Impurity -\n",
    "Measures the proportions of classes in a set: \n",
    "$Gini(t) = 1 - \\sum_{i=1}^{j}P(i|t)^2$ **,**  \n",
    "where *j* is the number of classes, *t* is the subset of instances for the node, \n",
    "and *P(i|t)* is the probability of selecting an element of class *i* from the\n",
    "node subset. Gini impurity is equal to zero when all the elements of the set \n",
    "are the same class. It is greatest when each class has an equal proability of\n",
    "being seleted.\n",
    "\n",
    "$Gini_max = 1 - \\frac{1}{n}$ **,** where n is the number of possible classes.\n",
    "\n",
    "data for the problem below can be found\n",
    "[here](http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2698: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    9.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.889\n",
      "Best parameters set:\n",
      "tclf__max_depth: 155\n",
      "tclf__min_samples_leaf: 1\n",
      "tclf__min_samples_split: 3\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.98      0.98       720\n",
      "          1       0.87      0.89      0.88       100\n",
      "\n",
      "avg / total       0.97      0.97      0.97       820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using decision trees to block banner ads on web pages by\n",
    "# predicting whether an image is an ad or article content\n",
    "# Explanitory variables: Dimensions of the image,\n",
    "#   words from page's URL, words from images URL, image's alt text,\n",
    "#   image's anchor text, window of words surrounding image tag,\n",
    "#   and dimensions of the image.\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "df = pd.read_csv('./ad.data', header=None)\n",
    "\n",
    "explanatory_variable_columns = set(df.columns.values)\n",
    "explanatory_variable_columns.remove(len(df.columns.values)-1)\n",
    "response_variable_column = df[len(df.columns.values)-1] # Last column describes the classes\n",
    "\n",
    "# Encode advertisements as the positive class (1) and content as the \n",
    "# negative class (0). Missing values marked with ' *?' and replace with -1\n",
    "y = [1 if e == 'ad.' else 0 for e in response_variable_column]\n",
    "X = df[list(explanatory_variable_columns)].copy()\n",
    "X.replace(to_replace=' *?', value=-1, regex=True, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Create pipeline and instance of DecisionTreeClassifier\n",
    "# Set criterion to entropy to build tree using Information Gain\n",
    "pipeline = Pipeline([\n",
    "    ('clf', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    "parameters = {\n",
    "    'clf__max_depth': (150, 155, 160),\n",
    "    'clf__min_samples_split': (2, 3),\n",
    "    'clf__min_samples_leaf': (1, 2, 3)\n",
    "}\n",
    "\n",
    "# Set grid search to optimise the model's f1 score\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "print('Best score: %0.3f' % grid_search.best_score_)\n",
    "print('Best parameters set:')\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "predictions = grid_search.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Advantages and Disadvantages of Decision Trees\n",
    "* Advantages\n",
    "  * Easy to use and do not require the data to be standardized\n",
    "  * Can tolerate missing values for features (although not yet in scikit-learn)\n",
    "  * Can learn to ignore irrelevant features\n",
    "  * Can learn to determine which features are most useful\n",
    "  * Supports multi-output tasks\n",
    "  * Small decision tree can be easy to visualize\n",
    "  * Single decision tree - renders multi-class classification\n",
    "* Disadvantages\n",
    "  * More prone to overfitting than many other models\n",
    "  * Learning algorithms can produce large, complicated decision trees\n",
    "  * Perfectly models every training instance, but fails to generalize the real relationship\n",
    "  * Pruning removes some of the tallest nodes and leaves (but not supported in scikit-learn)\n",
    "    * Can do pre-pruning by setting max depth for tree\n",
    "    * Creating child nodes only when training instances exceeds a threshold\n",
    "  * Other strategies for getting around complicated trees:\n",
    "    * Create multiple decision trees\n",
    "    \n",
    "#### ID3\n",
    "* Efficient decision tree learning algorithms are greedy\n",
    "* Learns efficiently by making locally optimal decisions\n",
    "* Constructs a tree by selecting a sequence of features to test\n",
    "* Feature selected reduces the uncertainty of the node\n",
    "* Locally sub-optimal tests may be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
