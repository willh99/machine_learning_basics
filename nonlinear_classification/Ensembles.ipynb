{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating)\n",
    "\n",
    "* **Bagging** is an ensemble meta-algorithm that reduces variance in an estimator\n",
    "* It can be used in classification and regression  classification tasks\n",
    "  * When components are **regressors**, the ensemble averages their predictions\n",
    "  * When components are **classifiers**, the ensemble returns the mode class\n",
    "* Independently fits multiple models on variants of training data\n",
    "* Training data variants are created using a technique called bootstrap resampling\n",
    "\n",
    "##### Bootstrap Resampling\n",
    "* A method of estimating the uncertainty in a statistic\n",
    "* Bootstrap resampling can only be used if observations in the sample are drawn independently\n",
    "* Produces multiple variants by resampling repeatedly from the original sample\n",
    "* Variant samples will have same number of observations as original sample\n",
    "* Statistics can be computed for each variant\n",
    "* Statistics can be used to estimate uncertainty by:\n",
    "  * Creating a confidence interval\n",
    "  * Calculating the standard error\n",
    "  \n",
    "Bagging is particularly useful for estimators that have high variance and low bias - decision trees  \n",
    "Bagged decision trees are used often and successfully called random forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample: [88 14 95 58 17 80 25 35 43 46]\n",
      "Sample mean: 50.1\n",
      "Number of bootstrap re-samples: 100\n",
      "Example re-sample: [43 46 58 14 43 88 58 88 25 88]\n",
      "Mean of re-samples' means: 50.87700000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample 10 integers\n",
    "sample = np.random.randint(low=1, high=100, size=10)\n",
    "print('Original sample: %s' % sample)\n",
    "print('Sample mean: %s' % sample.mean())\n",
    "\n",
    "# Bootstrap re-sample 100 times by re-sampling with replacement from original sample\n",
    "resamples = [np.random.choice(sample, size=sample.shape) for i in range(100)]\n",
    "print('Number of bootstrap re-samples: %s' % len(resamples))\n",
    "print('Example re-sample: %s' % resamples[0])\n",
    "\n",
    "resample_means = np.array([resample.mean() for resample in resamples])\n",
    "print('Mean of re-samples\\' means: %s' % resample_means.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "* The implementation of bagging on decision trees\n",
    "* Number of trees is an important hyperparameter\n",
    "* Increasing number of trees increases the model's preformance and computational complexity\n",
    "* Algorithm selects the best from random features at each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.66      0.69       127\n",
      "          1       0.68      0.75      0.71       123\n",
      "\n",
      "avg / total       0.71      0.70      0.70       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training a Random Forest with Scikit-Learn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create artificial classification set and split the test data\n",
    "# Dataset has 1000 instances with 100 features of which 20 are informative\n",
    "#     while the rest are redundent combinations of information features or noise.\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=100, n_informative=20, n_clusters_per_class=2, random_state=11)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=11)\n",
    "\n",
    "# Train and evaluate a single decision tree\n",
    "clf = DecisionTreeClassifier(random_state=11)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.83      0.79       127\n",
      "          1       0.80      0.70      0.75       123\n",
      "\n",
      "avg / total       0.77      0.77      0.77       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now train and evaluate on a random forest with 10 trees\n",
    "clf = RandomForestClassifier(n_estimators=10, random_state=11)\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
