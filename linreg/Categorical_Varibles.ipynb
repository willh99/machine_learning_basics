{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Variables\n",
    "\n",
    "Categorical Variables can take one of a fixed set of values. They often use one-hot encoding in which the eplainitory variable is represented using a binary feature representing one of its possible values.\n",
    "\n",
    "* Example: Represent cities with one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "onehot_encoder = DictVectorizer()\n",
    "\n",
    "X = [\n",
    "    {'city': 'New York'},\n",
    "    {'city': 'San Francisco'},\n",
    "    {'city': 'Chapel Hill'}\n",
    "]\n",
    "print(onehot_encoder.fit_transform(X).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.70710678 -1.38873015  0.52489066  0.59299945 -1.35873244]\n",
      " [ 0.         -0.70710678  0.46291005  0.87481777  0.81537425  1.01904933]\n",
      " [ 0.          1.41421356  0.9258201  -1.39970842 -1.4083737   0.33968311]]\n"
     ]
    }
   ],
   "source": [
    "# Standardization is needed so that one feature's variance does not cause it to\n",
    "# dominate over the learning algorithm.\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0., 0., 5., 13., 9., 1.],\n",
    "    [0., 0., 13., 15., 10., 15.],\n",
    "    [0., 3., 15., 2., 0., 11.]\n",
    "])\n",
    "print(preprocessing.scale(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- StandardScaler works by subtracting the mean of a feature from each instance's value and then dividing by the feature's standard deviation\n",
    "\n",
    "- RobustScaler is an alternative which subtracts the median and divides by  the interquartile range. Quartiles are calulated by splitting sorted datasets into four parts of equal size. Second quartile is the median while the Interquartile range is the difference of the third and first quartiles. This mitigates the effect of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRACTING FEATURES FROM TEXT\n",
    "\n",
    "* Bag-of-Words-Model: The most common representation of text, this model uses a multiset or \"bag\" that encodes the words that appear in a text.  It does not encode the text's syntax, ignores the order of words, and disregards all grammar. Can be though of as an extension of one-hot encoding. Creates one feature for each work of interest in the text. The intuition is that documents containing similar words have similar meanings. Can be used or document classification and retrieval. A collection of documents is called a corpus.\n",
    "\n",
    "  * Example: Using a corpus with 2 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/will/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/will/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/will/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/will/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 1 0 1 0 1]\n",
      " [1 1 1 0 1 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 1, 'in': 3, 'basketball': 0, 'lost': 4, 'the': 6, 'game': 2}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'UNC played Duke in basketball',\n",
    "    'Duke lost the basketball game'\n",
    "]\n",
    "\n",
    "# Number of elements is called a vector's dimension. Here, 8 unique words make a vector\n",
    "# with 8 elements. Maps the vocabulary to indicies in the feature vector (i.e. a dictionary)\n",
    "# CountVectorizer converts the characters to lowercase and tokenizes the documents.\n",
    "# Tokenization is done using regular expressions, splitting strings on whitespace and extracts\n",
    "# sequences of characters that are two or more characters in length.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 1 0 1 0 0 1]\n",
      " [0 1 1 1 0 1 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 1 0 0]]\n",
      "{'unc': 9, 'played': 6, 'duke': 2, 'in': 4, 'basketball': 1, 'lost': 5, 'the': 8, 'game': 3, 'ate': 0, 'sandwich': 7}\n"
     ]
    }
   ],
   "source": [
    "corpus.append('I ate a sandwich')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the first two feature vectors are closer to one another than the third vector. They will be more closely related using a metric such as euclidean distance.\n",
    "\n",
    "*Euclidean distance* b/w two vectors is equivalent to the Euclidean norm/L2 norm of the difference b/w two vecors. A norm is a fuction that assigns a positive size to a vector. Euclidean norm = vector's magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance between 1st and 2nd documents: [[2.44948974]]\n",
      "Distance between 1st and 3rd documents: [[2.64575131]]\n",
      "Distance between 2nd and 3rd documents: [[2.64575131]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "X = vectorizer.fit_transform(corpus).todense()\n",
    "print('Distance between 1st and 2nd documents:', euclidean_distances(X[0], X[1]))\n",
    "print('Distance between 1st and 3rd documents:', euclidean_distances(X[0], X[2]))\n",
    "print('Distance between 2nd and 3rd documents:', euclidean_distances(X[1], X[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic stategy for reducing dimension is converting text to lower case as it does not affect the meaning of the word **Stop Word Filtering** uses a second strategy to remove common words in the corpus. Examples include determiners (the, a, an), auxiliary verbs (do, be, will), and prepositions (on, around, beneath). These words are known as stop words. These are functional words that contribute to a text's grammar/meaning. However grammar is not considered by the bag-or-words-model.\n",
    "\n",
    "#### Example: stop word filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0 1 0 1]\n",
      " [0 1 1 1 1 0 0 0]\n",
      " [1 0 0 0 0 0 1 0]]\n",
      "{'unc': 7, 'played': 5, 'duke': 2, 'basketball': 1, 'lost': 4, 'game': 3, 'ate': 0, 'sandwich': 6}\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stemming and Lemmatization:** used to condense inflected and derived words into a single feature.\n",
    "\n",
    "Notice how the below documents have similar meanings, but their vectors have nothing in common as 'ate' and 'eaten' are treated seporately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "{'ate': 0, 'sandwichs': 3, 'sandwich': 2, 'eaten': 1}\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'He ate the sandwichs',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "vectorizer = CountVectorizer(binary=True, stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__Lemmatization__ is the process of determining the lemma, the derivative root, of an inflected word based on context. **Stemming** is similar but more simplistic as it does not attempt to produce the morphological root of words. Lemmatization often requires a lexical resource while stemming uses rules.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n",
      "gathering\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "corpus =  [\n",
    "    'I am gathering ingredients for the sandwich.',\n",
    "    'There were many wizards at the gathering'\n",
    "]\n",
    "print(lemmatizer.lemmatize('gathering', 'v'))\n",
    "print(lemmatizer.lemmatize('gathering', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gather\n"
     ]
    }
   ],
   "source": [
    "# Notice how PorterStemmer cannot make the distinction between the various from of the word\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('gathering'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed: [['He', 'ate', 'the', 'sandwich'], ['everi', 'sandwich', 'wa', 'eaten', 'by', 'him']]\n",
      "Lemmatized: [['He', 'eat', 'the', 'sandwich'], ['Every', 'sandwich', 'be', 'eat', 'by', 'him']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "wordnet_tags = ['n', 'v']\n",
    "corpus = [\n",
    "    'He ate the sandwichs',\n",
    "    'Every sandwich was eaten by him'\n",
    "]\n",
    "stemmer = PorterStemmer()\n",
    "print('Stemmed:', [[stemmer.stem(token) for token in word_tokenize(document)] for document in corpus])\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    if tag[0].lower() in ['n', 'v']:\n",
    "        return lemmatizer.lemmatize(token, tag[0].lower())\n",
    "    return token\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tagged_corpus = [pos_tag(word_tokenize(document)) for document in corpus]\n",
    "print('Lemmatized:', [[lemmatize(token, tag) for token, tag in document] for document in tagged_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3 1 1]\n",
      "Token indices {'dog': 1, 'ate': 0, 'sandwich': 2, 'wizard': 4, 'transfigured': 3}\n",
      "The token \"dog\" appears 1 times\n",
      "The token \"ate\" appears 2 times\n",
      "The token \"sandwich\" appears 3 times\n",
      "The token \"wizard\" appears 1 times\n",
      "The token \"transfigured\" appears 1 times\n"
     ]
    }
   ],
   "source": [
    "# Frequency of a word could denote its significance.\n",
    "# Extending bag-of-words with tf-idf weights (showing number of occurances)\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = ['The dog ate a sandwich, the wizard transfigured a sandwich, and I ate a sandwich']\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "frequencies = np.array(vectorizer.fit_transform(corpus).todense())[0]\n",
    "print(frequencies)\n",
    "print('Token indices %s' % vectorizer.vocabulary_)\n",
    "for token, index in vectorizer.vocabulary_.items():\n",
    "    print('The token \"%s\" appears %s times' % (token, frequencies[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.75458397 0.37729199 0.53689271 0.         0.        ]\n",
      " [0.         0.         0.44943642 0.6316672  0.6316672 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'The dog ate a sandwich and I ate a sandwich',\n",
    "    'The wizard transfigured a sandwich'\n",
    "]\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "print(vectorizer.fit_transform(corpus).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hashing trick - creating a dictionary for the corpus has two drawbacks:\n",
    "1. two passes are required over the corpus to create dictionary and feature vectors\n",
    "2. Dictionary must be stored in memory which is expensive for large corpora\n",
    "  * Solution: use hasing to map to memory locations instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0. -1.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "corpus = ['the', 'ate', 'bacon', 'cat']\n",
    "vectorizer = HashingVectorizer(n_features=6)\n",
    "print(vectorizer.transform(corpus).todense())\n",
    "\n",
    "# Errors from hash collisions cancel each other out with signage.\n",
    "# Resulting model is difficult to inspect since not stored in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings: \n",
    "Representations of text that mitigate shortcoming of bag-of-words model. Use a vector rather than a scalar to represent each token. Vectors are dense and have between 50 and 500 dimensions. Semantically similar words are represented by vectors near each other.\n",
    "\n",
    "See [this link](https:radimrehurk.com/gensim/install.html) for gensim installation instructions\n",
    "\n",
    "Download and gunzip the word2vec embedings from https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bf7b49d888c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# The model is large\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',\n\u001b[1;32m      5\u001b[0m                                                         binary=True)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# The model is large\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',\n",
    "                                                        binary=True)\n",
    "\n",
    "# Let's inspect the embedding for \"cat\"\n",
    "embedding = model.wor_vec('cat')\n",
    "print(\"Dimensions: %s\" % embedding.shape)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# See which words are more similar\n",
    "print(model.similarity('cat', 'dog'))\n",
    "print(model.similarity('cat', 'sandwich'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict similar words\n",
    "print(model.most_similar(positive=['puppy', 'cat'], negative=['kitten'], topn=3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
